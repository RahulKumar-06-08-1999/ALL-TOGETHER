{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOTXBu92M+4O3dCcLUweUib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAHULRAANU/ALL-TOGETHER/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter"
      ],
      "metadata": {
        "id": "OIuN6mlMr0R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n97FtycnSxD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import swifter\n",
        "import re\n",
        "import string\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading path \n",
        "path1 = r'/content/cybersecurity_tweets.csv'\n",
        "path2 = r'/content/not_cybersecurity_tweets.csv'"
      ],
      "metadata": {
        "id": "F0EK-GaTnck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv1 = pd.read_csv(path1)\n",
        "csv2 = pd.read_csv(path2)\n",
        "\n",
        "df = pd.concat([csv1,csv2])"
      ],
      "metadata": {
        "id": "e1ELpcSkn8-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "oYxx-V3Hn-0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = shuffle(df)\n",
        "df"
      ],
      "metadata": {
        "id": "r9avAMoeoBGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text\n",
        "\n"
      ],
      "metadata": {
        "id": "6sZWLbBAoBJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "UcVdBfQFoBMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n"
      ],
      "metadata": {
        "id": "gYklm0CfoO9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class text_preprocess:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def convert_to_lower(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def remove_emojis(self, text):\n",
        "        text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \" \", text) #remove links and mentions\n",
        "        text = re.sub(r\"<.*?>\",\" \",text)\n",
        "\n",
        "        wierd_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emotions\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u'\\U00010000-\\U0010ffff'\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u2640-\\u2642\"\n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\u3030\"\n",
        "            u\"\\ufe0f\"\n",
        "            u\"\\u2069\"\n",
        "            u\"\\u2066\"\n",
        "            u\"\\u200c\"\n",
        "            u\"\\u2068\"\n",
        "            u\"\\u2067\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        rm_emoji = wierd_pattern.sub(r'', text)\n",
        "        return rm_emoji\n",
        "\n",
        "    def remove_html(self, text):\n",
        "        html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "        rm_html = re.sub(html, ' ', text)\n",
        "        return rm_html\n",
        "\n",
        "    def remove_URL(self,text):\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        URL = url.sub(r' ', text)\n",
        "        return URL\n",
        "    \n",
        "    def remove_non_ascii(self, text):\n",
        "        return re.sub(r'[^\\x00-\\x7f]',r' ', text) # or ''.join([x for x in text if x in string.printable]) \n",
        "    \n",
        "    \n",
        "    def remove_numbers(self, text):\n",
        "        number_pattern = r'\\d+'\n",
        "        without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
        "        return without_number\n",
        "\n",
        "\n",
        "    def remove_punctuation(self,text):\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "    def remove_extra_white_spaces(self, text):\n",
        "        single_char_pattern = re.compile(r'\\s+[a-zA-Z]\\s+')\n",
        "        without_sc = re.sub(single_char_pattern, r\" \", text)\n",
        "#         without_sc = text.replace(' ', '')\n",
        "        return without_sc\n",
        "\n",
        "\n",
        "    def preprocessText(self,text):            \n",
        "        return self.remove_extra_white_spaces(self.remove_non_ascii(self.remove_URL(self.remove_html(self.remove_punctuation(self.remove_numbers(self.remove_emojis(self.convert_to_lower(text))))))))\n"
      ],
      "metadata": {
        "id": "a5apQlxfoBOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":    \n",
        "    text_prpocess_obj = text_preprocess()\n",
        "    df.text = df.text.swifter.apply(lambda x: text_prpocess_obj.preprocessText(x))"
      ],
      "metadata": {
        "id": "BVmj3ebCoBRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text"
      ],
      "metadata": {
        "id": "22Jym6p7oBT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "cFAjhvQxoaDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")  # giving installation error"
      ],
      "metadata": {
        "id": "AL4J4-kkoZKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the tweet base texts.\n",
        "def tokenize(text):\n",
        "    my_doc = nlp(text)\n",
        "    token_list = []\n",
        "    for token in my_doc:\n",
        "        token_list.append(token.text)\n",
        "    return token_list    "
      ],
      "metadata": {
        "id": "BsTS3fr-oBWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.swifter.apply(lambda x: tokenize(x))"
      ],
      "metadata": {
        "id": "rQu49Jt4oh0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text"
      ],
      "metadata": {
        "id": "IjWcyXrloh3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Stopwords"
      ],
      "metadata": {
        "id": "bQZU4-vKooUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):    \n",
        "    filtered_sentence =[] \n",
        "    for word in text:\n",
        "        lexeme = nlp.vocab[word]\n",
        "        if lexeme.is_stop == False:\n",
        "            filtered_sentence.append(word) \n",
        "    return \" \".join(filtered_sentence)"
      ],
      "metadata": {
        "id": "dHvmfBf0oh5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.swifter.apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "drCa6hWSoh8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text"
      ],
      "metadata": {
        "id": "oO4nyRFvoh-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "V9wiPfMgoxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = set(nltk.corpus.words.words())\n",
        "# words = nltk.word_tokenize(corpus)\n",
        "\n",
        "class lemmatization:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def lemmatizing_space(self, text):   \n",
        "        return \" \".join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
        "\n",
        "    # def lemmatizing_words(self, text):\n",
        "    #     return \" \".join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "    # def lemmatize(self, text):\n",
        "    #     return self.lemmatizing_space(self.lemmatizing_words(text))"
      ],
      "metadata": {
        "id": "fjeYzKqEx4sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  lemmatization_obj = lemmatization()\n",
        "  df.text = df.text.swifter.apply(lambda x: lemmatization_obj.lemmatizing_space(x))"
      ],
      "metadata": {
        "id": "B5VO0uECylVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "So9fWqWRESJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "xWqA_edQyoXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Stemming\n",
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# snow_stemmer = SnowballStemmer(language='english')\n",
        "  \n",
        "# def stemmizing(text):    \n",
        "#     #stem of each word\n",
        "#     stem_words = []\n",
        "#     for w in text:\n",
        "#         x = snow_stemmer.stem(w)\n",
        "#         stem_words.append(x)\n",
        "#     return \"\".join(stem_words)"
      ],
      "metadata": {
        "id": "FEFo-uJpoiBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['text'] = df.text.swifter.apply(lambda x: stemmizing(x))"
      ],
      "metadata": {
        "id": "JYHfqFf8pDJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.text"
      ],
      "metadata": {
        "id": "fVSvnilapDaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Remove words from a string of length between 2"
      ],
      "metadata": {
        "id": "8ewiExFzo2VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removelt2wordslength(text):    \n",
        "    for x in text:\n",
        "        xx = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "        rm_word = re.sub(xx, '', text)\n",
        "        return rm_word"
      ],
      "metadata": {
        "id": "zsq32e3HouCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.swifter.apply(lambda x: removelt2wordslength(x))"
      ],
      "metadata": {
        "id": "LCGHp_miouEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text"
      ],
      "metadata": {
        "id": "-dQCUYQpouHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lstm\n",
        "We will not going to create RNN model due to its vanishing gradient problem instead of that we will going to create LSTM model.LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. First of all we are going to do tokenization then we will generate sequence of n-grams.After that we will going to do padding.Padding is required because all the sentences are of different length so we need to make them of same length.We will going to do this by adding 0 in the end of the text with the help of pad_sequences function of keras\n"
      ],
      "metadata": {
        "id": "7ChxBKc2pN0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "ihr8d891ouJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "NFXts2IHqURd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 100000\n",
        "max_len = 19\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(df['text'].values)\n",
        "sequences = tok.texts_to_sequences(df['text'].values)\n",
        "sequences_matrix = pad_sequences(sequences, padding = 'post', maxlen= max_len)\n"
      ],
      "metadata": {
        "id": "AJ3xOuJWqced"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_matrix"
      ],
      "metadata": {
        "id": "1qrVCGHhqcg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df[['label']]"
      ],
      "metadata": {
        "id": "RPEzGQbJqcjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "A5drWxtxqcmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix,Y, test_size = 0.27, random_state = 2529 ,stratify = Y)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "TE01CuGbqcrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lstm architecture\n",
        "# Embedding : Generates embedding vector for each input sequence\n",
        "# Dense : Fully connected layer for classification\n",
        "# bidirectional : Another type of rnn simultaneously learn forward and backword direction of information flow\n",
        "# Lstm : long short term memory, its a variant of RNN which has memory state cell to learn the context of words are at the further along the text to carry contextual meaning rahther than just neighbouring words as in case of rnn"
      ],
      "metadata": {
        "id": "StiQJLgGa7P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout\n",
        "\n",
        "embid_dim = 512\n",
        "lstm_out = 128\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Embedding(max_words, embid_dim, input_length = sequences_matrix.shape[1]))\n",
        "model.add(Bidirectional(LSTM(lstm_out)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(512, activation = 'relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "irogG6sqqczN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer= Adam(learning_rate = 0.01), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AzyHnJVFqc1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(X_train,Y_train,epochs=10, validation_data= (X_test, Y_test),\n",
        "#           callbacks=[EarlyStopping(monitor='val_loss',min_delta=0, patience=3, verbose=1, mode='auto' )])\n",
        "\n",
        "# batch_size = 64\n",
        "# earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "# history = model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1, validation_data= (X_test, Y_test),callbacks=[earlystop])\n",
        "\n",
        "batch_size = 128\n",
        "earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "history = model.fit(X_train, Y_train, epochs = 11, batch_size=batch_size, verbose = 1, validation_data= (X_test, Y_test),callbacks=[earlystop])"
      ],
      "metadata": {
        "id": "rU9GSh51qc4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "# Plot Accuracy and Loss"
      ],
      "metadata": {
        "id": "Zj6Udm2FrKlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tgFLU0Tzqc7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score"
      ],
      "metadata": {
        "id": "BoDaiPaSsVKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = model.predict(X_test)\n",
        "Y_pred"
      ],
      "metadata": {
        "id": "BkX57LQnvMGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred[0]"
      ],
      "metadata": {
        "id": "CqGygOpevxSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = accuracy_score(Y_test,Y_pred.round())\n",
        "test_accuracy"
      ],
      "metadata": {
        "id": "w_zARdCQsViG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(Y_test,Y_pred.round())\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "2cgTCDQ3qqeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap of confusion matrix\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(confusion_matrix(Y_test, Y_pred.round()), annot=True, fmt = '1d');"
      ],
      "metadata": {
        "id": "NGx3NEsWqqje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification report\n",
        "print(classification_report(Y_test,  Y_pred.round()))"
      ],
      "metadata": {
        "id": "sqncukFUxVY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "the model will ouput a prediction score between 0 and 1 . we can classify two classes by defining a threshold value for it. in our case i have set 0.5 as Threshold value, if the score above it, Then it will classified as \n",
        "Cybersecurity text"
      ],
      "metadata": {
        "id": "fGKAy8NRRH4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_text(score):\n",
        "  return \"cybersecurity_text\" if score > 0.50 else \"Not_cybersecurity_text\"\n",
        "\n",
        "score = model.predict(X_test)  \n",
        "print(len(score))"
      ],
      "metadata": {
        "id": "iyoEdJjDxVbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred.round()"
      ],
      "metadata": {
        "id": "Z06sQx-QY9r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y_pred.mean() # for threshold"
      ],
      "metadata": {
        "id": "xLgk-0m-XBwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_text = [decode_text(score) for score in score]\n",
        "# y_pred_text"
      ],
      "metadata": {
        "id": "glJ9ZdL2xVeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "4tvE69BjUOeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokk = Tokenizer()"
      ],
      "metadata": {
        "id": "UBodEdCnUPzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =  ['Threat: Someone with the potential to cause harm by damaging or destroying the official data of a system or organization.']"
      ],
      "metadata": {
        "id": "sFEM9SQVOjsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokk.fit_on_texts(text)\n",
        "seq = tokk.texts_to_sequences(text)\n",
        "seqmatrix = pad_sequences(seq, padding = 'post', maxlen= max_len)"
      ],
      "metadata": {
        "id": "opIZfy2mOyy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = model.predict(seqmatrix)"
      ],
      "metadata": {
        "id": "d8iHdBy8Oy1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "JU1Kq9ZkRUxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_text_other = [decode_text(score) for score in test]"
      ],
      "metadata": {
        "id": "e501Cs5KQCZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_text_other"
      ],
      "metadata": {
        "id": "u66i6qeVQR9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lstm.h5')"
      ],
      "metadata": {
        "id": "7qNCC2vFWNeR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}